Penetration testing is a security assessment methodology. Since security can apply to multiple domain (webapp, mobile, infrastructure, ...), it seems logical that we have multiple tools, multiple frameworks!

With version 3, the OSSTMM encompasses
tests from all channels - Human, Physical, Wireless, Telecommunications, and Data Networks. This also makes it
a perfectly suited for testing cloud computing, virtual infrastructures, messaging middleware, mobile
communication infrastructures, high-security locations, human resources, trusted computing, and any logical
processes which all cover multiple channels and require a different kind of security test. A set of attack surface
metrics, called ravs, provide a powerful and highly flexible tool that can provide a graphical representation of
state, and show changes in state over time. This integrates well with a ’dashboard’ for management and is
beneficial for both internal and external testing, allowing a comparison/combination of the two.

***In this document, an OSSTMM
audit or “audit” is the result of the analysis performed after an OSSTMM test. The
person who performs this function of test and analysis is referred to as the Security***

A set of attack surface
metrics, called ravs, provide a powerful and highly flexible tool that can provide a graphical representation of
state, and show changes in state over time. 

Purpose
The primary purpose of this manual is to provide a scientific methodology for the accurate
characterization of operational security (OpSec) through examination and correlation of test results in a
consistent and reliable way. This manual is adaptable to almost any audit type, including penetration
tests, ethical hacking, security assessments, vulnerability assessments, red-teaming, blue-teaming, and so
forth. It is written as a security research document and is designed for factual security verification and
presentation of metrics on a professional level.
A secondary purpose is to provide guidelines which, when followed correctly, will allow the analyst to
perform a certified OSSTMM audit. These guidelines exist to assure the following:
1. The test was conducted thoroughly.
2. The test included all necessary channels.
3. The posture for the test complied with the law.
4. The results are measurable in a quantifiable way.
5. The results are consistent and repeatable.
6. The results contain only facts as derived from the tests themselves.
An indirect benefit of this manual is that it can act as a central reference in all security tests regardless of
the size of the organization, technology, or protection.
Document Scope
The scope of this document is to provide specific descriptions for operational security tests over all
operational channels, which include Human, Physical, Wireless, Telecommunications, and Data Networks,
over any vector, and the description of derived metrics. This manual only focuses on OpSec and the use
of the words safety and security are within this context.
Liability
This manual describes certain tests which are designed to elicit a response. Should these tests cause harm
or damage, the Analyst may be liable according to the laws governing the Analyst’s location as well as
the location of the tested systems. ISECOM makes no guarantee as to a harmless outcome of any test.
Any Analyst applying this methodology cannot hold ISECOM liable for problems which arise during
testing. In using this methodology, the Analyst agrees to assume this liability. 

- Therefore, under the context of operational security, we call
- security: the separation of an asset and a threat 
- safety: the control of a threat or its effects.
- To better understand how OpSec can work within an operational environment, it must be reduced to its
elements. These elements allow one to quantify the Attack Surface, which is the lack of specific
separations and functional controls that exist for that Vector, the direction of the interaction. The
reductionist approach resolves to us needing to see security and safety in a new way, one that allows for
them to exist independent of risk and fully capable of creating Perfect Security, the exact balance of
security and controls with operations and limitations.

read pg 22-table


- So consider that if the separation exists properly from the threats, such as a man inside a mountain
avoiding lightning, then that security is true; it is 100%. For every hole in the mountain, every means for
lightning to cause harm to that man, the porosity increases as an Access. Each point of interaction
reduces the security below 100%, where 100% represents a full separation. Therefore, the increase in
porosity is the decrease in security and each pore is either a Visibility, Access, or Trust.

- The inability of protection mechanisms to work are their limitations.



- With economics like this, it is little wonder why software vendors move from solely performing black-box security testing, which can only be performed on applications that have already been developed, to concentrating on testing in the early cycles of application development, such as during definition, design, and development.




# Penetration Testing Methodologies
Summary
OWASP Testing Guides
Web Security Testing Guide (WSTG)
Mobile Security Testing Guide (MSTG)
Firmware Security Testing Methodology
Penetration Testing Execution Standard
PCI Penetration Testing Guide
PCI DSS Penetration Testing Guidance
PCI DSS Penetration Testing Requirements
Penetration Testing Framework
Technical Guide to Information Security Testing and Assessment
Open Source Security Testing Methodology Manual
References
OWASP Testing Guides
In terms of technical security testing execution, the OWASP testing guides are highly recommended. Depending on the types of the applications, the testing guides are listed below for the web/cloud services, Mobile app (Android/iOS), or IoT firmware respectively.

OWASP Web Security Testing Guide
OWASP Mobile Security Testing Guide
OWASP Firmware Security Testing Methodology
Penetration Testing Execution Standard
Penetration Testing Execution Standard (PTES) defines penetration testing as 7 phases. Particularly, PTES Technical Guidelines give hands-on suggestions on testing procedures, and recommendation for security testing tools.

Pre-engagement Interactions
Intelligence Gathering
Threat Modeling
Vulnerability Analysis
Exploitation
Post Exploitation
Reporting
PTES Technical Guidelines

PCI Penetration Testing Guide
Payment Card Industry Data Security Standard (PCI DSS) Requirement 11.3 defines the penetration testing. PCI also defines Penetration Testing Guidance.

PCI DSS Penetration Testing Guidance
The PCI DSS Penetration testing guideline provides guidance on the following:

Penetration Testing Components
Qualifications of a Penetration Tester
Penetration Testing Methodologies
Penetration Testing Reporting Guidelines
PCI DSS Penetration Testing Requirements
The PCI DSS requirement refer to Payment Card Industry Data Security Standard (PCI DSS) Requirement 11.3

Based on industry-accepted approaches
Coverage for CDE and critical systems
Includes external and internal testing
Test to validate scope reduction
Application-layer testing
Network-layer tests for network and OS
PCI DSS Penetration Test Guidance


# What Is the OWASP Testing Methodology?
Security testing will never be an exact science where a complete list of all possible issues that should be tested can be defined. In fact, security testing is only one of the several suitable techniques for testing the security of web applications under certain circumstances. The goal of this project is to collect all the possible testing techniques, explain these techniques, and keep the guide updated. The OWASP Web Application Security Testing methodology is based on the black box approach. The tester has little to no information about the application to be tested.

The testing model consists of:

Tester: Who performs the testing activities
Tools and methodology: The core of this Testing Guide project
Application: The black box to test
Testing can be categorized as passive or active:

Passive Testing
During passive testing, a tester tries to understand the application's logic and explores the application as an end user. Tools can be used for information gathering. For example, an HTTP(S) proxy can be used to observe all the HTTP(S) requests and responses. At the end of this phase, the tester should generally understand all the access points and functionality of the system (e.g., HTTP headers, parameters, cookies, APIs, technology usage/patterns, etc). The Information Gathering section explains how to perform passive testing.

For example, a tester may find a page at the following URL: https://www.example.com/login/auth_form

This may indicate an authentication form where the application requests a username and password.

The following parameters represent two access points to the application: https://www.example.com/appx?a=1&b=1

In this case, the application has two access points (parameters a and b). All the input points found in this phase represent targets for testing. Keeping track of the directory or call tree of the application and all the access points can be useful during active testing.

Active Testing
During active testing, a tester uses the methodologies described in the following sections.

The set of active tests have been split into 12 categories:

Information Gathering
Configuration and Deployment Management Testing
Identity Management Testing
Authentication Testing
Authorization Testing
Session Management Testing
Input Validation Testing
Testing for Error Handling
Testing for Weak Cryptography
Business Logic Testing
Client-side Testing
API Testing


# Search Engines
Do not limit testing to just one search engine provider, as different search engines may generate different results. Search engine results can vary in a few ways, depending on when the engine last crawled content, and the algorithm the engine uses to determine relevant pages. Consider using the following (alphabetically listed) search engines:

Baidu, China's most popular search engine.
Bing, a search engine owned and operated by Microsoft, and the second most popular worldwide. Supports advanced search keywords.
binsearch.info, a search engine for binary Usenet newsgroups.
Common Crawl, "an open repository of web crawl data that can be accessed and analyzed by anyone."
DuckDuckGo, a privacy-focused search engine that compiles results from many different sources. Supports search syntax.
Google, which offers the world's most popular search engine, and uses a ranking system to attempt to return the most relevant results. Supports search operators.
Internet Archive Wayback Machine, "building a digital library of internet sites and other cultural artifacts in digital form."
Shodan, a service for searching internet-connected devices and services. Usage options include a limited free plan as well as paid subscription plans.
Search Operators
A search operator is a special keyword or syntax that extends the capabilities of regular search queries, and can help obtain more specific results. They generally take the form of operator:query. Here are some commonly supported search operators:

site: will limit the search to the provided domain.
inurl: will only return results that include the keyword in the URL.
intitle: will only return results that have the keyword in the page title.
intext: or inbody: will only search for the keyword in the body of pages.
filetype: will match only a specific file type, i.e. .png, or .php.
For example, to find the web content of owasp.org as indexed by a typical search engine, the syntax required is:

site:owasp.org
Google Site Operation Search Result Example
Figure 4.1.1-1: Google Site Operation Search Result Example

Viewing Cached Content
To search for content that has previously been indexed, use the cache: operator. This is helpful for viewing content that may have changed since the time it was indexed, or that may no longer be available. Not all search engines provide cached content to search; the most useful source at time of writing is Google.

To view owasp.org as it is cached, the syntax is:

cache:owasp.org
Google Cache Operation Search Result Example
Figure 4.1.1-2: Google Cache Operation Search Result Example

Google Hacking or Dorking
Searching with operators can be a very effective discovery technique when combined with the creativity of the tester. Operators can be chained to effectively discover specific kinds of sensitive files and information. This technique, called Google hacking or Dorking, is also possible using other search engines, as long as the search operators are supported.

A database of dorks, like the Google Hacking Database, is a useful resource that can help uncover specific information. Some categories of dorks available on this database include:

Footholds
Files containing usernames
Sensitive Directories
Web Server Detection
Vulnerable Files
Vulnerable Servers
Error Messages
Files containing juicy info
Files containing passwords
Sensitive Online Shopping Info



# Fingerprint Web Server
ID
WSTG-INFO-02
Summary
Web server fingerprinting is the task of identifying the type and version of web server that a target is running on. While web server fingerprinting is often encapsulated in automated testing tools, it is important for researchers to understand the fundamentals of how these tools attempt to identify software, and why this is useful.

Accurately discovering the type of web server that an application runs on can enable security testers to determine if the application is vulnerable to attack. In particular, servers running older versions of software without up-to-date security patches can be susceptible to known version-specific exploits.

Test Objectives
Determine the version and type of a running web server to enable further discovery of any known vulnerabilities.
How to Test
Techniques used for web server fingerprinting include banner grabbing, eliciting responses to malformed requests, and using automated tools to perform more robust scans that use a combination of tactics. The fundamental premise on which all these techniques operate is the same. They all strive to elicit some response from the web server which can then be compared to a database of known responses and behaviors, and thus matched to a known server type.

Banner Grabbing
A banner grab is performed by sending an HTTP request to the web server and examining its response header. This can be accomplished using a variety of tools, including telnet for HTTP requests, or openssl for requests over TLS/SSL.

For example, here is the response to a request sent to an Apache server.

HTTP/1.1 200 OK
Date: Thu, 05 Sep 2019 17:42:39 GMT
Server: Apache/2.4.41 (Unix)
Last-Modified: Thu, 05 Sep 2019 17:40:42 GMT
ETag: "75-591d1d21b6167"
Accept-Ranges: bytes
Content-Length: 117
Connection: close
Content-Type: text/html
...
Here is another response, this time sent by nginx.

HTTP/1.1 200 OK
Server: nginx/1.17.3
Date: Thu, 05 Sep 2019 17:50:24 GMT
Content-Type: text/html
Content-Length: 117
Last-Modified: Thu, 05 Sep 2019 17:40:42 GMT
Connection: close
ETag: "5d71489a-75"
Accept-Ranges: bytes
...
Here's what a response sent by lighttpd looks like.

HTTP/1.0 200 OK
Content-Type: text/html
Accept-Ranges: bytes
ETag: "4192788355"
Last-Modified: Thu, 05 Sep 2019 17:40:42 GMT
Content-Length: 117
Connection: close
Date: Thu, 05 Sep 2019 17:57:57 GMT
Server: lighttpd/1.4.54
In these examples, the server type and version is clearly exposed. However, security-conscious applications may obfuscate their server information by modifying the header. For example, here is an excerpt from the response to a request for a site with a modified header:

HTTP/1.1 200 OK
Server: Website.com
Date: Thu, 05 Sep 2019 17:57:06 GMT
Content-Type: text/html; charset=utf-8
Status: 200 OK
...
In cases where the server information is obscured, testers may guess the type of server based on the ordering of the header fields. Note that in the Apache example above, the fields follow this order:

Date
Server
Last-Modified
ETag
Accept-Ranges
Content-Length
Connection
Content-Type
However, in both the nginx and obscured server examples, the fields in common follow this order:

Server
Date
Content-Type
Testers can use this information to guess that the obscured server is nginx. However, considering that a number of different web servers may share the same field ordering and fields can be modified or removed, this method is not definite.

Sending Malformed Requests
Web servers may be identified by examining their error responses, and in the cases where they have not been customized, their default error pages. One way to compel a server to present these is by sending intentionally incorrect or malformed requests.



For example, here is the response to a request for the non-existent method SANTA CLAUS from an Apache server.

GET / SANTA CLAUS/1.1


HTTP/1.1 400 Bad Request
Date: Fri, 06 Sep 2019 19:21:01 GMT
Server: Apache/2.4.41 (Unix)
Content-Length: 226
Connection: close
Content-Type: text/html; charset=iso-8859-1

<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML 2.0//EN">
<html><head>
<title>400 Bad Request</title>
</head><body>
<h1>Bad Request</h1>
<p>Your browser sent a request that this server could not understand.<br />
</p>
</body></html>
Here is the response to the same request from nginx.

GET / SANTA CLAUS/1.1


<html>
<head><title>404 Not Found</title></head>
<body>
<center><h1>404 Not Found</h1></center>
<hr><center>nginx/1.17.3</center>
</body>
</html>
Here is the response to the same request from lighttpd.

GET / SANTA CLAUS/1.1


HTTP/1.0 400 Bad Request
Content-Type: text/html
Content-Length: 345
Connection: close
Date: Sun, 08 Sep 2019 21:56:17 GMT
Server: lighttpd/1.4.54

<?xml version="1.0" encoding="iso-8859-1"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
         "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
 <head>
  <title>400 Bad Request</title>
 </head>
 <body>
  <h1>400 Bad Request</h1>
 </body>
</html>
As default error pages offer many differentiating factors between types of web servers, their examination can be an effective method for fingerprinting even when server header fields are obscured.

Using Automated Scanning Tools
As stated earlier, web server fingerprinting is often included as a functionality of automated scanning tools. These tools are able to make requests similar to those demonstrated above, as well as send other more server-specific probes. Automated tools can compare responses from web servers much faster than manual testing, and utilize large databases of known responses to attempt server identification. For these reasons, automated tools are more likely to produce accurate results.

Here are some commonly-used scan tools that include web server fingerprinting functionality.

Netcraft, an online tool that scans sites for information, including web server details.
Nikto, an Open Source command-line scanning tool.
Nmap, an Open Source command-line tool that also has a GUI, Zenmap.
Remediation
While exposed server information is not necessarily in itself a vulnerability, it is information that can assist attackers in exploiting other vulnerabilities that may exist. Exposed server information can also lead attackers to find version-specific server vulnerabilities that can be used to exploit unpatched servers. For this reason it is recommended that some precautions be taken. These actions include:

Obscuring web server information in headers, such as with Apache's mod_headers module.
Using a hardened reverse proxy server to create an additional layer of security between the web server and the internet.
Ensuring that web servers are kept up-to-date with the latest software and security patches.

read point 4-information gathering



# Review Webserver Metafiles for Information Leakage
ID
WSTG-INFO-03
Summary
This section describes how to test various metadata files for information leakage of the web application's path(s), or functionality. Furthermore, the list of directories that are to be avoided by Spiders, Robots, or Crawlers can also be created as a dependency for Map execution paths through application. Other information may also be collected to identify attack surface, technology details, or for use in social engineering engagement.

Test Objectives
Identify hidden or obfuscated paths and functionality through the analysis of metadata files.
Extract and map other information that could lead to a better understanding of the systems at hand.
How to Test
Any of the actions performed below with wget could also be done with curl. Many Dynamic Application Security Testing (DAST) tools such as ZAP and Burp Suite include checks or parsing for these resources as part of their spider/crawler functionality. They can also be identified using various Google Dorks or leveraging advanced search features such as inurl:.

Robots
Web Spiders, Robots, or Crawlers retrieve a web page and then recursively traverse hyperlinks to retrieve further web content. Their accepted behavior is specified by the Robots Exclusion Protocol of the robots.txt file in the web root directory.

As an example, the beginning of the robots.txt file from Google sampled on 2020 May 5 is quoted below:

User-agent: *
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
Disallow: /sdch
...
The User-Agent directive refers to the specific web spider/robot/crawler. For example, the User-Agent: Googlebot refers to the spider from Google while User-Agent: bingbot refers to a crawler from Microsoft. User-Agent: * in the example above applies to all web spiders/robots/crawlers.

The Disallow directive specifies which resources are prohibited by spiders/robots/crawlers. In the example above, the following are prohibited:

...
Disallow: /search
...
Disallow: /sdch
...
Web spiders/robots/crawlers can intentionally ignore the Disallow directives specified in a robots.txt file. Hence, robots.txt should not be considered as a mechanism to enforce restrictions on how web content is accessed, stored, or republished by third parties.

The robots.txt file is retrieved from the web root directory of the web server. For example, to retrieve the robots.txt from www.google.com using wget or curl:

$ curl -O -Ss http://www.google.com/robots.txt && head -n5 robots.txt
User-agent: *
Disallow: /search
Allow: /search/about
Allow: /search/static
Allow: /search/howsearchworks
...
Analyze robots.txt Using Google Webmaster Tools
Site owners can use the Google "Analyze robots.txt" function to analyze the site as part of its Google Webmaster Tools. This tool can assist with testing and the procedure is as follows:

Sign into Google Webmaster Tools with a Google account.
On the dashboard, enter the URL for the site to be analyzed.
Choose between the available methods and follow the on screen instruction.
META Tags
<META> tags are located within the HEAD section of each HTML document and should be consistent across a site in the event that the robot/spider/crawler start point does not begin from a document link other than webroot i.e. a deep link. The Robots directive can also be specified using a specific META tag.

Robots META Tag
If there is no <META NAME="ROBOTS" ... > entry, then the "Robots Exclusion Protocol" defaults to INDEX,FOLLOW respectively. Therefore, the other two valid entries defined by the "Robots Exclusion Protocol" are prefixed with NO... i.e. NOINDEX and NOFOLLOW.

Based on the Disallow directive(s) listed within the robots.txt file in webroot, a regular expression search for <META NAME="ROBOTS" is undertaken within each web page. The result is then compared to the robots.txt file in the webroot.

Miscellaneous META Information Tags
Organizations often embed informational META tags in web content to support various technologies such as screen readers, social networking previews, search engine indexing, etc. Such meta-information can be of value to testers in identifying technologies used, and additional paths/functionality to explore and test. The following meta information was retrieved from www.whitehouse.gov via View Page Source on 2020 May 05:

...
<meta property="og:locale" content="en_US" />
<meta property="og:type" content="website" />
<meta property="og:title" content="The White House" />
<meta property="og:description" content="We, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all. – President Donald Trump." />
<meta property="og:url" content="https://www.whitehouse.gov/" />
<meta property="og:site_name" content="The White House" />
<meta property="fb:app_id" content="1790466490985150" />
<meta property="og:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta property="og:image:secure_url" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:description" content="We, the citizens of America, are now joined in a great national effort to rebuild our country and to restore its promise for all. – President Donald Trump." />
<meta name="twitter:title" content="The White House" />
<meta name="twitter:site" content="@whitehouse" />
<meta name="twitter:image" content="https://www.whitehouse.gov/wp-content/uploads/2017/12/wh.gov-share-img_03-1024x538.png" />
<meta name="twitter:creator" content="@whitehouse" />
...
<meta name="apple-mobile-web-app-title" content="The White House">
<meta name="application-name" content="The White House">
<meta name="msapplication-TileColor" content="#0c2644">
<meta name="theme-color" content="#f5f5f5">
...
Sitemaps
A sitemap is a file where a developer or organization can provide information about the pages, videos, and other files offered by the site or application, and the relationship between them. Search engines can use this file to navigate your site more efficiently. Likewise, testers can utilize 'sitemap.xml' files to gain deeper insights into the site or application under investigation.

The following excerpt is from Google's primary sitemap retrieved 2020 May 05.

$ wget --no-verbose https://www.google.com/sitemap.xml && head -n8 sitemap.xml
2020-05-05 12:23:30 URL:https://www.google.com/sitemap.xml [2049] -> "sitemap.xml" [1]

<?xml version="1.0" encoding="UTF-8"?>
<sitemapindex xmlns="http://www.google.com/schemas/sitemap/0.84">
  <sitemap>
    <loc>https://www.google.com/gmail/sitemap.xml</loc>
  </sitemap>
  <sitemap>
    <loc>https://www.google.com/forms/sitemaps.xml</loc>
  </sitemap>
...
Exploring from there a tester may wish to retrieve the gmail sitemap https://www.google.com/gmail/sitemap.xml:

<?xml version="1.0" encoding="UTF-8"?>
<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9" xmlns:xhtml="http://www.w3.org/1999/xhtml">
  <url>
    <loc>https://www.google.com/intl/am/gmail/about/</loc>
    <xhtml:link href="https://www.google.com/gmail/about/" hreflang="x-default" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/el/gmail/about/" hreflang="el" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/it/gmail/about/" hreflang="it" rel="alternate"/>
    <xhtml:link href="https://www.google.com/intl/ar/gmail/about/" hreflang="ar" rel="alternate"/>
...
Security TXT
security.txt was ratified by the IETF as RFC 9116 - A File Format to Aid in Security Vulnerability Disclosure which allows sites to define security policies and contact details. There are multiple reasons why this might be of interest in testing scenarios, which include, but are not limited to:

Identifying further paths or resources to include in discovery/analysis.
Open Source intelligence gathering.
Finding information on Bug Bounties, etc.
Social Engineering.
The file may be present either in the root of the webserver or in the .well-known/ directory, for example:

https://example.com/security.txt
https://example.com/.well-known/security.txt
Here is a real world example retrieved from LinkedIn 2020 May 05:

$ wget --no-verbose https://www.linkedin.com/.well-known/security.txt && cat security.txt
2020-05-07 12:56:51 URL:https://www.linkedin.com/.well-known/security.txt [333/333] -> "security.txt" [1]
# Conforms to IETF `draft-foudil-securitytxt-07`
Contact: mailto:security@linkedin.com
Contact: https://www.linkedin.com/help/linkedin/answer/62924
Encryption: https://www.linkedin.com/help/linkedin/answer/79676
Canonical: https://www.linkedin.com/.well-known/security.txt
Policy: https://www.linkedin.com/help/linkedin/answer/62924
OpenPGP Public Keys contain some metadata that can provide information about the key itself. Here are some common metadata elements that can be extracted from an OpenPGP Public Key:

Key ID: The Key ID is a short identifier derived from the public key material. It helps identify the key and is often displayed as an eight-character hexadecimal value.
Key Fingerprint: The Key Fingerprint is a longer and more unique identifier derived from the key material. It is often displayed as a 40-character hexadecimal value. Key fingerprints are commonly used to verify the integrity and authenticity of a public key.
Key Algorithm: The Key Algorithm represents the cryptographic algorithm used by the public key. OpenPGP supports various algorithms such as RSA, DSA, and ECC (Elliptic Curve Cryptography).
Key Size: The Key Size refers to the length or size of the cryptographic key in bits. It indicates the strength of the key and determines the level of security provided by the key.
Key Creation Date: The Key Creation Date indicates when the key was generated or created.
Key Expiration Date: OpenPGP Public Keys can have an expiration date set, after which they are considered invalid. The Key Expiration Date specifies when the key is no longer valid.
User IDs: Public keys can have one or more associated User IDs that identify the owner or entity associated with the key. User IDs typically include information such as the name, email address, and optional comments of the key owner.
Humans TXT
humans.txt is an initiative for knowing the people behind a site. It takes the form of a text file that contains information about the different people who have contributed to building the site. This file often (but not always) contains information related to career or job sites/paths.

The following example was retrieved from Google 2020 May 05:

$ wget --no-verbose  https://www.google.com/humans.txt && cat humans.txt
2020-05-07 12:57:52 URL:https://www.google.com/humans.txt [286/286] -> "humans.txt" [1]
Google is built by a large team of engineers, designers, researchers, robots, and others in many different sites across the globe. It is updated continuously, and built with more tools and technologies than we can shake a stick at. If you'd like to help us out, see careers.google.com.
Other .well-known Information Sources
There are other RFCs and internet drafts which suggest standardized uses of files within the .well-known/ directory. Lists of these can be found here or here.

It would be fairly simple for a tester to review the RFC/drafts and create a list to be supplied to a crawler or fuzzer, in order to verify the existence or content of such files.

Tools
Browser (View Source or Dev Tools functionality)
curl
wget
Burp Suite
ZAP



